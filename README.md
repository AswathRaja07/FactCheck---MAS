# ğŸ›¡ï¸ Multi-Agent Fact Verification System using Qdrant (MAS Track â€“ Convolve 4.0)

[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.0+-red.svg)](https://streamlit.io/)
[![Qdrant](https://img.shields.io/badge/Qdrant-Cloud-orange.svg)](https://qdrant.tech/)

This project is submitted for **Round-2 of Convolve 4.0 â€” Multi-Agent Intelligent Systems (MAS) Track â€“ Qdrant**.

---

## ğŸ“‹ Table of Contents

- [ğŸ§  Problem Statement](#-problem-statement)
- [ğŸ¯ Proposed Solution](#-proposed-solution)
- [ğŸ“š Dataset Used](#-dataset-used)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Installation & Setup](#-installation--setup)
- [ğŸ“– Usage](#-usage)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“Š Data Format](#-data-format)
- [ğŸ¤– Agent Details](#-agent-details)
- [ğŸ¨ UI Features](#-ui-features)
- [ğŸ” Technical Details](#-technical-details)
- [ğŸ“ˆ Performance Considerations](#-performance-considerations)
- [ğŸ¤ Contributing](#-contributing)
- [ğŸ“„ License](#-license)
- [ğŸ™ Acknowledgments](#-acknowledgments)

---

## ğŸ§  Problem Statement

Misinformation spreads rapidly online without verification. The goal is to build an **intelligent system** that:

- âœ… Accepts a natural-language claim
- âœ… Searches real fact-check databases
- âœ… Retrieves relevant evidence
- âœ… Generates a verdict (+ confidence)
- âœ… Presents output in a clean UI

The system provides verdicts such as: **âœ” VERIFIED** | **âŒ DEBUNKED** | **â“ UNKNOWN**

---

## ğŸ¯ Proposed Solution

I designed and implemented a **Multi-Agent Intelligent System** with the following agents:

### 1. ğŸ§¹ Claim Normalization Agent
Cleans user input for better semantic matching.

### 2. ğŸ§  Embedding Agent
Converts text â†’ dense vector using `all-MiniLM-L6-v2`.

### 3. ğŸ” Evidence Retrieval Agent (Qdrant)
Uses **Approximate Nearest Neighbor (ANN)** search to retrieve fact-checked evidence.

### 4. âš–ï¸ Verdict Agent
Calculates label ratios, applies score thresholds, and determines final verdict.

### 5. ğŸ“Š UI Reporting Agent
Displays verdict, confidence scores, evidence, and charts.

---

## ğŸ“š Dataset Used

| Dataset | Description | Source |
|---------|-------------|--------|
| **PolitiFact Fact-Check Dataset** | Human-verified fact checks | [Kaggle Dataset](https://www.kaggle.com/datasets/liushaoping/politifact-factchecking-dataset/data) |
| **Verified Scientific Facts** | Curated verified scientific claims | `data/verified_facts.json` |

---

## ğŸ—ï¸ Architecture

### System Components

```mermaid
graph TD
    A[User Input] --> B[Claim Normalization Agent]
    B --> C[Embedding Agent]
    C --> D[Evidence Retrieval Agent]
    D --> E[Verdict Agent]
    E --> F[UI Reporting Agent]
    F --> G[User Output]
```

### Core Components

- **ğŸ–¥ï¸ Streamlit App** (`app.py`): Web interface for user interaction
- **ğŸ¤– Agents** (`agents.py`): Multi-agent pipeline implementation
- **âš™ï¸ Configuration** (`config.py`): Qdrant connection settings
- **ğŸ“¥ Data Ingestion Scripts**:
  - `create_collection.py`: Creates Qdrant collection
  - `upsert_factchecks.py`: Uploads PolitiFact data
  - `upsert_verified.py`: Uploads verified scientific facts

### Agent Workflow

1. **Input Claim** â†’ Claim Normalization Agent
2. **Normalized Text** â†’ Embedding Agent â†’ Vector
3. **Vector** â†’ Evidence Retrieval Agent â†’ Top-K Similar Evidence
4. **Evidence** â†’ Verdict Agent â†’ Final Verdict + Confidence
5. **Results** â†’ UI Reporting Agent â†’ Display to User

---

## ğŸš€ Installation & Setup

### Prerequisites
- Python 3.8+
- Qdrant Cloud account (or local Qdrant instance)
- Internet connection for downloading models

### Step 1: Clone & Install Dependencies
```bash
git clone <repository-url>
cd FactCheck-MAS
pip install -r requirements.txt
```

### Step 2: Configure Qdrant
Edit `config.py` with your Qdrant credentials:
```python
QDRANT_URL = "https://19f65b30-a9ee-4dfe-a464-7ab559058c66.us-east4-0.gcp.cloud.qdrant.io"
QDRANT_API_KEY = "***"
COLLECTION_NAME = "verified_facts_text"
```

### Step 3: Download Dataset
- Download PolitiFact dataset from Kaggle
- Save as `politifact_factcheck_data.jsonl` in project root
- Create `data/verified_facts.json` with curated verified facts

### Step 4: Setup Qdrant Collection
```bash
python create_collection.py
```

### Step 5: Upload Data
```bash
python upsert_factchecks.py  # Upload PolitiFact data
python upsert_verified.py    # Upload verified facts
```

---

## ğŸ“– Usage

### Running the Application
```bash
streamlit run app.py
```

### How to Use
1. Open the provided URL in your browser
2. Enter a claim in the text box
3. Click "Verify Claim"
4. View the verdict, confidence scores, and evidence

### Example Claims to Test
- "COVID vaccines cause infertility"
- "The Earth is flat"
- "Water boils at 100Â°C at sea level"

---

## ğŸ“ Project Structure

```
FactCheck-MAS/
â”œâ”€â”€ ğŸ“„ agents.py              # Multi-agent implementation
â”œâ”€â”€ ğŸŒ app.py                 # Streamlit web application
â”œâ”€â”€ âš™ï¸ config.py              # Configuration settings
â”œâ”€â”€ ğŸ—„ï¸ create_collection.py   # Qdrant collection creation
â”œâ”€â”€ ğŸ“¤ upsert_factchecks.py   # PolitiFact data ingestion
â”œâ”€â”€ ğŸ“¤ upsert_verified.py     # Verified facts ingestion
â”œâ”€â”€ ğŸ“‹ requirements.txt       # Python dependencies
â”œâ”€â”€ ğŸ“– README.md             # This documentation
â”œâ”€â”€ ğŸ“Š politifact_factcheck_data.jsonl  # Dataset (download required)
â””â”€â”€ ğŸ“ data/
    â””â”€â”€ ğŸ“„ verified_facts.json  # Curated verified facts
```

---

## ğŸ”§ Configuration

### Qdrant Settings
| Parameter | Value | Description |
|-----------|-------|-------------|
| Vector Size | 384 | Matches all-MiniLM-L6-v2 |
| Distance Metric | Cosine | Similarity measurement |
| Collection Name | `verified_facts_text` | Database collection |

### Agent Parameters
| Parameter | Value | Description |
|-----------|-------|-------------|
| Top-K Retrieval | 3 | Number of evidence points |
| Similarity Threshold | 0.60 | Minimum similarity score |
| Verdict Ratio Threshold | 0.70 | Minimum ratio for verdict |

---

## ğŸ“Š Data Format

### Verified Facts JSON Format
```json
[
  {
    "text": "The Earth orbits the Sun once every 365.25 days",
    "source": "NASA",
    "analysis_link": "https://science.nasa.gov/mission/earth/"
  }
]
```

### PolitiFact Data Fields
- `statement`: The claim text
- `verdict`: PolitiFact rating (True, False, Pants on Fire, etc.)
- `factcheck_analysis_link`: Source URL

---

## ğŸ¤– Agent Details

### ğŸ§¹ Claim Normalization Agent
- Lowercases text
- Removes punctuation
- Strips whitespace

### ğŸ§  Embedding Agent
- Uses SentenceTransformer `all-MiniLM-L6-v2`
- Outputs 384-dimensional vectors

### ğŸ” Evidence Retrieval Agent
- Queries Qdrant with cosine similarity
- Returns top-3 most similar evidence points

### âš–ï¸ Verdict Agent
- Filters evidence by similarity threshold (0.60)
- Counts verified vs debunked labels
- Applies ratio-based decision logic:
  - â‰¥70% debunked â†’ **DEBUNKED**
  - â‰¥70% verified â†’ **VERIFIED**
  - Otherwise â†’ **UNKNOWN**

---

## ğŸ¨ UI Features

- **ğŸ† Verdict Display**: Clear visual indicators (âœ… âŒ â“)
- **ğŸ“Š Confidence Scores**: Bar chart of evidence distribution
- **ğŸ“‹ Evidence List**: Top retrieved fact-checks with sources
- **ğŸ¯ Similarity Scores**: Cosine similarity values

---

## ğŸ” Technical Details

| Component | Technology | Details |
|-----------|------------|---------|
| Vector Database | Qdrant Cloud | Scalable vector search |
| Embedding Model | all-MiniLM-L6-v2 | 384D sentence embeddings |
| Similarity Metric | Cosine Distance | Semantic similarity |
| Frontend | Streamlit | Interactive web app |
| Language | Python 3.8+ | Core implementation |

---

## ğŸ“ˆ Performance Considerations

- **ğŸ“Š Indexing**: ~10,000 fact-checks indexed
- **âš¡ Query Latency**: <1 second for embedding + retrieval
- **ğŸ¯ Accuracy**: Depends on dataset quality and similarity thresholds
- **ğŸ“ˆ Scalability**: Qdrant handles millions of vectors efficiently

---

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Make changes and test thoroughly
4. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
5. Push to the branch (`git push origin feature/AmazingFeature`)
6. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **ğŸ“° PolitiFact** for the fact-checking dataset
- **ğŸ—„ï¸ Qdrant** for the vector database
- **ğŸ§  SentenceTransformers** for embedding models
- **ğŸŒ Streamlit** for the web framework
- **ğŸ† Convolve 4.0** for the MAS Track challenge
